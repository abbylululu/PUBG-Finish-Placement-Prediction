{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train_V2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_X = pd.read_csv('test_V2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Dealing with Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train.dropna()\n",
    "df_train['winPlacePerc'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Dealing with Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guoshumao/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Concatenating two dataset in order to do data preprocessing in both sets\n",
    "df = pd.concat([df_train,test_X],ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detecting and dropping players' data with extreme killing counts\n",
    "df.drop(df[df['kills'] > 30].index, inplace=True)\n",
    "df.drop(df[df['roadKills'] > 10].index, inplace=True)\n",
    "\n",
    "# detecting and dropping the killing without moving playes' data\n",
    "df['totalDistance'] = df['rideDistance'] + df['walkDistance'] + df['swimDistance']\n",
    "df['killsWithoutMoving'] = ((df['kills'] > 0) & (df['totalDistance'] == 0))\n",
    "df.drop(df[df['killsWithoutMoving'] == True].index, inplace=True)\n",
    "\n",
    "# detecting and dropping players' data with extreme long distance kills\n",
    "df.drop(df[df['longestKill'] >= 1000].index, inplace=True)\n",
    "\n",
    "# detecting and dropping players' data with extreme moving distance\n",
    "df.drop(df[df['walkDistance'] >= 10000].index, inplace=True)\n",
    "df.drop(df[df['rideDistance'] >= 20000].index, inplace=True)\n",
    "df.drop(df[df['swimDistance'] >= 2000].index, inplace=True)\n",
    "\n",
    "# deleting extreme damageDealt data\n",
    "df.drop(df[df['damageDealt'] >= (np.percentile(df['damageDealt'], 99))].index, inplace=True)\n",
    "\n",
    "# deleting extreme acquired weapons and heals data\n",
    "df.drop(df[df['weaponsAcquired'] >= (np.percentile(df['weaponsAcquired'], 99))].index, inplace=True)\n",
    "df.drop(df[df['heals'] >= (np.percentile(df['heals'], 99))].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# drop unuseful features\n",
    "df = df.drop(['killsWithoutMoving'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Normalizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning object values into categorical values and get category coding\n",
    "df['groupId'] = df['groupId'].astype('category')\n",
    "df['matchId'] = df['matchId'].astype('category')\n",
    "\n",
    "df['groupId_cat'] = df['groupId'].cat.codes\n",
    "df['matchId_cat'] = df['matchId'].cat.codes\n",
    "\n",
    "df.drop(columns=['groupId', 'matchId'], inplace=True)\n",
    "\n",
    "# Because the test set contains different Ids, Id column won't be useful to the machine learning algorithm\n",
    "df.drop(columns = ['Id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns = ['matchType'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Splitting Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4276555, 44)\n",
      "(1862905, 44)\n"
     ]
    }
   ],
   "source": [
    "# split the train and test set in order to do EDA\n",
    "df_train_pro = df[~df['winPlacePerc'].isnull()]\n",
    "test_X_pro = df[df['winPlacePerc'].isnull()]\n",
    "print(df_train_pro.shape)\n",
    "print(test_X_pro.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_pro = test_X_pro.drop(['winPlacePerc'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/guoshumao/anaconda3/lib/python3.6/site-packages (0.4.1)\n",
      "Requirement already satisfied: torchvision in /Users/guoshumao/anaconda3/lib/python3.6/site-packages (0.2.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /Users/guoshumao/anaconda3/lib/python3.6/site-packages (from torchvision) (5.1.0)\n",
      "Requirement already satisfied: numpy in /Users/guoshumao/anaconda3/lib/python3.6/site-packages (from torchvision) (1.14.3)\n",
      "Requirement already satisfied: six in /Users/guoshumao/anaconda3/lib/python3.6/site-packages (from torchvision) (1.11.0)\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's get rid of some imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#Define the model \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2993588, 43) (1282967, 43) (2993588,) (1282967,)\n"
     ]
    }
   ],
   "source": [
    "#This will throw and error at import if haven't upgraded. \n",
    "# from sklearn.cross_validation  import train_test_split  \n",
    "from sklearn.model_selection  import train_test_split\n",
    "#y is the dependent variable.\n",
    "y = df_train_pro['winPlacePerc']\n",
    "#As we know, iloc is used to slice the array by index number. Here this is the matrix of \n",
    "#independent variables.\n",
    "X = df_train_pro.iloc[:,0:43]\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define training hyperprameters.\n",
    "batch_size = 50\n",
    "num_epochs = 200\n",
    "learning_rate = 0.01\n",
    "size_hidden= 100\n",
    "\n",
    "#Calculate some other hyperparameters based on data.  \n",
    "batch_no = len(X_train) // batch_size  #batches\n",
    "cols=X_train.shape[1] #Number of columns in input matrix\n",
    "n_output=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing the model on : cpu\n"
     ]
    }
   ],
   "source": [
    "#Create the model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
    "print(\"Executing the model on :\",device)\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, size_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(cols, size_hidden)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(size_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n",
    "net = Net(cols, size_hidden, n_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guoshumao/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "#Adam is a specific flavor of gradient decent which is typically better\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.2)\n",
    "criterion = torch.nn.MSELoss(size_average=False)  # this is for regression mean squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change to numpy arraay. \n",
    "X_train=X_train.values\n",
    "y_train=y_train.values\n",
    "X_test=X_test.values\n",
    "y_test=y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss:  1052536289502.3394\n",
      "Epoch 2 loss:  296830709.6415849\n",
      "Epoch 3 loss:  276931.01168203354\n",
      "Epoch 4 loss:  276921.15700507164\n",
      "Epoch 5 loss:  276922.1037712097\n",
      "Epoch 6 loss:  276915.3274667263\n",
      "Epoch 7 loss:  276922.0236167908\n",
      "Epoch 8 loss:  276905.35077905655\n",
      "Epoch 9 loss:  276929.7626488209\n",
      "Epoch 10 loss:  276911.1831550598\n",
      "Epoch 11 loss:  276900.1951057911\n",
      "Epoch 12 loss:  276913.04455804825\n",
      "Epoch 13 loss:  276926.94372558594\n",
      "Epoch 14 loss:  276896.97618603706\n",
      "Epoch 15 loss:  276928.90104842186\n",
      "Epoch 16 loss:  276934.49557590485\n",
      "Epoch 17 loss:  276909.43905878067\n",
      "Epoch 18 loss:  276921.11840867996\n",
      "Epoch 19 loss:  276918.04668188095\n",
      "Epoch 20 loss:  276920.5670449734\n",
      "Epoch 21 loss:  276912.0158083439\n",
      "Epoch 22 loss:  276929.18187737465\n",
      "Epoch 23 loss:  276917.5998892784\n",
      "Epoch 24 loss:  276907.48375701904\n",
      "Epoch 25 loss:  276922.4272685051\n",
      "Epoch 26 loss:  276911.09183335304\n",
      "Epoch 27 loss:  276890.95586538315\n",
      "Epoch 28 loss:  276903.0332374573\n",
      "Epoch 29 loss:  276897.92585778236\n",
      "Epoch 30 loss:  276924.79251646996\n",
      "Epoch 31 loss:  276914.7994618416\n",
      "Epoch 32 loss:  276927.49454164505\n",
      "Epoch 33 loss:  276932.6960155964\n",
      "Epoch 34 loss:  276900.5207147598\n",
      "Epoch 35 loss:  276900.8575208187\n",
      "Epoch 36 loss:  276931.2584707737\n",
      "Epoch 37 loss:  276914.4032006264\n",
      "Epoch 38 loss:  276913.47392368317\n",
      "Epoch 39 loss:  276918.63912534714\n",
      "Epoch 40 loss:  276922.11093211174\n",
      "Epoch 41 loss:  276913.5625913143\n",
      "Epoch 42 loss:  276921.311480999\n",
      "Epoch 43 loss:  276913.2819170952\n",
      "Epoch 44 loss:  276927.8920505047\n",
      "Epoch 45 loss:  276915.4038205147\n",
      "Epoch 46 loss:  276926.6293435097\n",
      "Epoch 47 loss:  276922.6612737179\n",
      "Epoch 48 loss:  276917.8899540901\n",
      "Epoch 49 loss:  276925.36257982254\n",
      "Epoch 50 loss:  276934.7333674431\n",
      "Epoch 51 loss:  276949.6279914379\n",
      "Epoch 52 loss:  276929.8033936024\n",
      "Epoch 53 loss:  276924.4972615242\n",
      "Epoch 54 loss:  276907.0572903156\n",
      "Epoch 55 loss:  276919.48486208916\n",
      "Epoch 56 loss:  276944.093228817\n",
      "Epoch 57 loss:  276902.4363274574\n",
      "Epoch 58 loss:  276920.54097795486\n",
      "Epoch 59 loss:  276896.9654791355\n",
      "Epoch 60 loss:  276915.1672434807\n",
      "Epoch 61 loss:  276909.28341293335\n",
      "Epoch 62 loss:  276928.38493800163\n",
      "Epoch 63 loss:  276928.4243617058\n",
      "Epoch 64 loss:  276922.9010784626\n",
      "Epoch 65 loss:  276913.26257252693\n",
      "Epoch 66 loss:  276929.47067976\n",
      "Epoch 67 loss:  276931.18342089653\n",
      "Epoch 68 loss:  276919.03072714806\n",
      "Epoch 69 loss:  276930.51823472977\n",
      "Epoch 70 loss:  276918.8491387367\n",
      "Epoch 71 loss:  276927.18651914597\n",
      "Epoch 72 loss:  276908.485042572\n",
      "Epoch 73 loss:  276918.2253780365\n",
      "Epoch 74 loss:  276898.08654761314\n",
      "Epoch 75 loss:  276939.6684560776\n",
      "Epoch 76 loss:  276930.02693605423\n",
      "Epoch 77 loss:  276913.02409410477\n",
      "Epoch 78 loss:  276924.0338740349\n",
      "Epoch 79 loss:  276908.96310710907\n",
      "Epoch 80 loss:  276939.09500718117\n",
      "Epoch 81 loss:  276927.4174349308\n",
      "Epoch 82 loss:  276915.1859278679\n",
      "Epoch 83 loss:  276951.2086992264\n",
      "Epoch 84 loss:  276933.7697122097\n",
      "Epoch 85 loss:  276915.8731033802\n",
      "Epoch 86 loss:  276946.22106671333\n",
      "Epoch 87 loss:  276919.7951402664\n",
      "Epoch 88 loss:  276937.4638853073\n",
      "Epoch 89 loss:  276927.2680399418\n",
      "Epoch 90 loss:  276896.24430918694\n",
      "Epoch 91 loss:  276938.2488629818\n",
      "Epoch 92 loss:  276923.3405351639\n",
      "Epoch 93 loss:  276918.6641910076\n",
      "Epoch 94 loss:  276912.64827895164\n",
      "Epoch 95 loss:  276912.03442549706\n",
      "Epoch 96 loss:  276920.6011669636\n",
      "Epoch 97 loss:  276906.0448832512\n",
      "Epoch 98 loss:  276924.3766539097\n",
      "Epoch 99 loss:  276926.16031622887\n",
      "Epoch 100 loss:  276925.92840337753\n",
      "Epoch 101 loss:  276913.228556633\n",
      "Epoch 102 loss:  276924.66732931137\n",
      "Epoch 103 loss:  276916.14783477783\n",
      "Epoch 104 loss:  276908.75988030434\n",
      "Epoch 105 loss:  276910.04003953934\n",
      "Epoch 106 loss:  276920.3203265667\n",
      "Epoch 107 loss:  276918.5220513344\n",
      "Epoch 108 loss:  276923.5743403435\n",
      "Epoch 109 loss:  276924.59386730194\n",
      "Epoch 110 loss:  276936.88051247597\n",
      "Epoch 111 loss:  276941.7612156868\n",
      "Epoch 112 loss:  276936.16552710533\n",
      "Epoch 113 loss:  276910.5641362667\n",
      "Epoch 114 loss:  276930.86715197563\n",
      "Epoch 115 loss:  276915.7886354923\n",
      "Epoch 116 loss:  276909.46343040466\n",
      "Epoch 117 loss:  276919.41872000694\n",
      "Epoch 118 loss:  276923.15547442436\n",
      "Epoch 119 loss:  276914.7969362736\n",
      "Epoch 120 loss:  276933.1503484249\n",
      "Epoch 121 loss:  276934.15956521034\n",
      "Epoch 122 loss:  276927.6675899029\n",
      "Epoch 123 loss:  276928.2074518204\n",
      "Epoch 124 loss:  276936.01015996933\n",
      "Epoch 125 loss:  276923.2718360424\n",
      "Epoch 126 loss:  276920.1099677086\n",
      "Epoch 127 loss:  276930.351829052\n",
      "Epoch 128 loss:  276920.0741491318\n",
      "Epoch 129 loss:  276910.1118209362\n",
      "Epoch 130 loss:  276895.9337887764\n",
      "Epoch 131 loss:  276907.74015688896\n",
      "Epoch 132 loss:  276924.74008488655\n",
      "Epoch 133 loss:  276919.6391093731\n",
      "Epoch 134 loss:  276911.5016992092\n",
      "Epoch 135 loss:  276942.7312145233\n",
      "Epoch 136 loss:  276928.80676960945\n",
      "Epoch 137 loss:  276918.3515422344\n",
      "Epoch 138 loss:  276930.6692035198\n",
      "Epoch 139 loss:  276921.1328215599\n",
      "Epoch 140 loss:  276925.0258307457\n",
      "Epoch 141 loss:  276909.18178987503\n",
      "Epoch 142 loss:  276953.31121587753\n",
      "Epoch 143 loss:  276928.3572688103\n",
      "Epoch 144 loss:  276905.13877749443\n",
      "Epoch 145 loss:  276919.2593536377\n",
      "Epoch 146 loss:  276908.6240899563\n",
      "Epoch 147 loss:  276920.44556355476\n",
      "Epoch 148 loss:  276904.94273900986\n",
      "Epoch 149 loss:  276936.85530138016\n",
      "Epoch 150 loss:  276942.1440169811\n",
      "Epoch 151 loss:  276922.7308986187\n",
      "Epoch 152 loss:  276917.1795632839\n",
      "Epoch 153 loss:  276918.56519031525\n",
      "Epoch 154 loss:  276922.8394777775\n",
      "Epoch 155 loss:  276923.7825603485\n",
      "Epoch 156 loss:  276908.321857214\n",
      "Epoch 157 loss:  276923.64035630226\n",
      "Epoch 158 loss:  276917.48665881157\n",
      "Epoch 159 loss:  276921.22207927704\n",
      "Epoch 160 loss:  276902.05469441414\n",
      "Epoch 161 loss:  276916.10343432426\n",
      "Epoch 162 loss:  276898.41266417503\n",
      "Epoch 163 loss:  276939.63606381416\n",
      "Epoch 164 loss:  276925.24515914917\n",
      "Epoch 165 loss:  276922.48690342903\n",
      "Epoch 166 loss:  276910.9864115715\n",
      "Epoch 167 loss:  276934.14584445953\n",
      "Epoch 168 loss:  276905.04531121254\n",
      "Epoch 169 loss:  276935.0992064476\n",
      "Epoch 170 loss:  276909.439150095\n",
      "Epoch 171 loss:  276932.48704981804\n",
      "Epoch 172 loss:  276927.0681877136\n",
      "Epoch 173 loss:  276918.74112558365\n",
      "Epoch 174 loss:  276912.5859427452\n",
      "Epoch 175 loss:  276889.168043375\n",
      "Epoch 176 loss:  276925.24970817566\n",
      "Epoch 177 loss:  276914.2580885887\n",
      "Epoch 178 loss:  276937.08142852783\n",
      "Epoch 179 loss:  276924.4975924492\n",
      "Epoch 180 loss:  276921.656093359\n",
      "Epoch 181 loss:  276902.5002038479\n",
      "Epoch 182 loss:  276932.37193083763\n",
      "Epoch 183 loss:  276910.488278389\n",
      "Epoch 184 loss:  276929.5379152298\n",
      "Epoch 185 loss:  276923.79281139374\n",
      "Epoch 186 loss:  276917.5363121033\n",
      "Epoch 187 loss:  276932.1491177082\n",
      "Epoch 188 loss:  276917.947565794\n",
      "Epoch 189 loss:  276924.95156502724\n",
      "Epoch 190 loss:  276930.66626143456\n",
      "Epoch 191 loss:  276922.96091890335\n",
      "Epoch 192 loss:  276909.9343318939\n",
      "Epoch 193 loss:  276911.89040613174\n",
      "Epoch 194 loss:  276905.94626045227\n",
      "Epoch 195 loss:  276925.50992155075\n",
      "Epoch 196 loss:  276905.87301278114\n",
      "Epoch 197 loss:  276922.36979722977\n",
      "Epoch 198 loss:  276925.4367671013\n",
      "Epoch 199 loss:  276905.07571315765\n",
      "Epoch 200 loss:  276926.2539360523\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from torch.autograd import Variable\n",
    "running_loss = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    #Shuffle just mixes up the dataset between epocs\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    # Mini batch learning\n",
    "    for i in range(batch_no):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        inputs = Variable(torch.FloatTensor(X_train[start:end]))\n",
    "        labels = Variable(torch.FloatTensor(y_train[start:end]))\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        #print(\"outputs\",outputs)\n",
    "        #print(\"outputs\",outputs,outputs.shape,\"labels\",labels, labels.shape)\n",
    "        loss = criterion(outputs, torch.unsqueeze(labels,dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print('Epoch {}'.format(epoch+1), \"loss: \",running_loss)\n",
    "    running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2993588 2993588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-95830221310.6652"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X = Variable(torch.FloatTensor(X_train)) \n",
    "result = net(X)\n",
    "pred=result.data[:,0].numpy()\n",
    "print(len(pred),len(y_train))\n",
    "r2_score(pred,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "#This is a little bit tricky to get the resulting prediction.  \n",
    "def calculate_r2(x,y=[]):\n",
    "    \"\"\"\n",
    "    This function will return the r2 if passed x and y or return predictions if just passed x. \n",
    "    \"\"\"\n",
    "    # Evaluate the model with the test set. \n",
    "    X = Variable(torch.FloatTensor(x))  \n",
    "    result = net(X) #This outputs the value for regression\n",
    "    result=result.data[:,0].numpy()\n",
    "  \n",
    "    if len(y) != 0:\n",
    "        r2=r2_score(result, y)\n",
    "        print(\"R-Squared\", r2)\n",
    "        #print('Accuracy {:.2f}'.format(num_right / len(y)), \"for a total of \", len(y), \"records\")\n",
    "        return pd.DataFrame(data= {'actual': y, 'predicted': result})\n",
    "    else:\n",
    "        print(\"returning predictions\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-Squared -95830221310.6652\n",
      "R-Squared -473592.1055717208\n"
     ]
    }
   ],
   "source": [
    "result1=calculate_r2(X_train,y_train)\n",
    "result2=calculate_r2(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinearRegression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm.fit( X_train, y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for Train) 1.0\n",
      "R2 for Test (cross validation) 1.0\n"
     ]
    }
   ],
   "source": [
    "print('R2 for Train)', lm.score( X_train, y_train ))\n",
    "print('R2 for Test (cross validation)', lm.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
